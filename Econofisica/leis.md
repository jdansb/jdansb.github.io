# Leis estatísticas, modelagem baseada em agentes e econofísica
Minha intenção nesta sessão é apenas fazer um apanhado geral de como a pesquisa funciona nessa área e, em parte, o que motiva sua existência. Para quem estiver mais interessado, além das leituras já indicadas na página inicial, praticamente qualquer livro da área começa buscando oferecer uma contextualização histórica do desenvolvimento do campo.

Podemos citar obras conhecidas, como por exemplo *Essentials of Econophysics Modelling* (Slanina) e *Econophysics: An Introduction* (Sinha, Chatterjee, Chakraborti e Chakrabarti), além da obra brasileira escrita por um economista: *[Economia da complexidade: Econofísica](https://fernandonogueiracosta.wordpress.com/wp-content/uploads/2024/09/fernando-nogueira-da-costa-economia-da-complexidade-ou-econofisica-ago-2024.pdf)* (Fernando Nogueira da Costa)[^1].

## Leis estatísticas

Começamos a discussão por uma perspectiva mais geral, abordando a relação entre leis estatísticas e sistemas complexos[^2]. Atualmente, existem pesquisas em diferentes áreas que tratam seus objetos de estudo como sistemas complexos, nos quais leis estatísticas aparecem como propriedades emergentes. Este é o caso da econofísica, em que entendemos o sistema econômico como um sistema complexo, do qual emergem leis estatísticas (também conhecidas como fatos estilizados), que estudaremos mais adiante. Apesar de muitas vezes serem vistas como desconexas, essas pesquisas costumam empregar uma mesma abordagem.

Esse padrão na forma como a investigação é conduzida em diferentes casos particulares é um sinal claro de que esse conjunto de casos, na verdade, constitui uma área de investigação dotada de uma metodologia científica própria. É de conhecimento universal a importância científica do processo de identificação de padrões — uma etapa fundamental da investigação na física, como é tradicionalmente compreendida. 

Assim, a primeira conexão que podemos estabelecer entre a econofísica e o modo tradicional de fazer ciência está na centralidade do processo de identificação de padrões. Quetelet, por exemplo, já no século XIX, buscou identificar padrões em dados relativos a nascimentos, idade de casamento, atividades criminosas e taxas de mortalidade.

Antes de continuarmos, é necessário definir o que é um sistema complexo: Sistemas complexos são sistemas compostos por vários componentes microscópicos que interagem entre si, dando origem a fenômenos não triviais em escalas macroscópicas. Esses fenômenos, que não podem ser encontrados nos componentes individuais do sistema, são chamados de fenômenos emergentes, ou seja, propriedades emergentes que surgem da interação dos componentes do sistema. Na física, podemos citar como exemplo os gases como um sistema complexo e a pressão como uma propriedade emergente. A pressão do gás não é apenas o somatório da "pressão contida" em cada partícula que o compõe, mas o resultado das interações entre as partículas que formam o gás.

Dentro do contexto de sistemas complexos, esses padrões costumam ser descritos não por leis deterministas, mas por leis estatísticas. Essas leis estatísticas, por sua vez, devem ser entendidas como propriedades emergentes com características estatísticas inerentes, que podem ser produzidas em escala macroscópica a partir de modelos microscópicos de sistemas complexos.

Uma lei estatística deve ter duas características para fazer jus ao nome.  Em primeiro lugar, deve ser uma lei matemática que diferentes sistemas exibam para realmente merecer o nome de *lei*.  Em segundo lugar, deve possuir necessariamente uma natureza estatística. Ou seja, a lei se manifesta em termos de grandezas estatísticas (por exemplo: distribuições, probabilidades, médias etc.), valores que só podem ser obtidos a partir de uma grande quantidade de medidas.

Tendo delimitado o objeto de estudo (sistemas complexos que apresentam uma lei estatística como propriedade emergente), podemos caracterizar um método de investigação padrão:  
1. A descrição, em um primeiro momento, dos padrões observados no sistema por meio de leis estatísticas;  
2. O desenvolvimento e a utilização, em um segundo momento, de modelos microscópicos capazes de gerar essas leis macroscópicas.

Esta forma de fazer ciência representa um novo paradigma na investigação da realidade social. As leis estatísticas são funções com poucos parâmetros, propostas para descrever um grande conjunto de dados em diferentes cenários.  Isso permite não apenas um resumo dos dados, mas também análises analíticas e abre a possibilidade para uma análise teórica.  Os estudos das leis estatísticas não devem ser encarados como meras curiosidades; a investigação dessas leis pretende revelar as propriedades mais fundamentais do sistema estudado.

Na tradição da física social, as leis estatísticas eram vistas como semelhantes às leis empíricas da física. Assim, o termo física social estava associado à busca por comportamentos análogos a leis na sociedade, e a ideia de que existiam leis que se relacionavam com a sociedade — tal como a mecânica de Newton se relacionava com o movimento dos planetas — era compartilhada por muitos no final do século XVII. A expectativa da física social era a de que ela evoluiria como disciplina de forma semelhante ao desenvolvimento historicamente construído da mecânica clássica: observações → leis empíricas → leis universais.


Ou seja, primeiro a comunidade realiza um grande conjunto de observações e consegue extrair diversas leis empíricas pontuais dessas observações. Em seguida, a partir dessas leis empíricas, é possível abstrair leis mais gerais, que explicam tanto as leis empíricas para diferentes casos particulares quanto os dados observados. Como exemplos de leis empíricas temos Kepler e Galileu; como lei universal, temos Newton.  Embora não haja uma definição muito clara, podemos pensar que a diferença reside no escopo: enquanto as leis empíricas têm uma validade mais limitada, uma lei universal nos fornece uma descrição mais fundamental e abrangente da realidade, sendo capaz de explicar diversas leis empíricas.

Nessa perspectiva, as leis estatísticas desempenhariam o papel de leis empíricas, a etapa intermediária crucial entre as observações empíricas e o desenvolvimento de leis universais. Porém, essa analogia simplista ignora a natureza estatística fundamental das leis estatísticas, que são essencialmente diferentes da lei de Kepler.  Ainda assim, um aspecto positivo dessa visão ingênua, mantido nas aplicações contemporâneas das leis estatísticas, é a expectativa de que elas conectem observações a modelos teóricos mais gerais — ainda que não necessariamente baseados em equações.

Este modelo mais geral pode ser tanto conceitual quanto computacional (por exemplo, um modelo baseado em agentes, conforme veremos adiante). Ocupando a posição de "lei universal", podemos buscar modelos microscópicos (ou conceituais) mais gerais que consigam reproduzir (ou explicar) não apenas uma única lei estatística, mas várias.

É principalmente o uso combinado das leis estatísticas e dos modelos microscópicos dos sistemas complexos que caracteriza a investigação da distribuição de riquezas na econofísica, da forma como é trabalhada hoje.  Há três mudanças que merecem ser destacadas na forma como se trabalhava com sociofísica e que foram causadas pela adesão à abordagem dos sistemas complexos:

1. Não ver as leis estatísticas como um simples efeito independente de influências aleatórias, que se espera encontrar em partes isoladas do sistema, mas sim como uma propriedade emergente de um sistema complexo.
2. O interesse nas leis vai além das aplicações práticas ou discussões filosóficas; elas são motivações e justificativas para a proposição de modelos mecanísticos dos fundamentos microscópicos do sistema.
3. A explicação teórica das leis não segue o paradigma da mecânica clássica, mas o da mecânica estatística, isto é, baseada em modelos probabilísticos.

### Formalização

Uma lei estatística (em sistemas complexos) é uma função que:
1. Foi proposta para descrever um grande número de observações em diferentes cenários (universalidade);
2. É elementar ou uma composição de funções elementares com um pequeno número de parâmetros e dimensões (simplicidade);
3. Desempenha um papel importante em uma teoria ou modelo (conexão teórica).

Tipicamente, uma lei estatística se aplica a um conjunto de dados e descreve a frequência de observação de alguma coisa ou a relação entre duas propriedades do sistema observado. A universalidade expressa nossa intenção de que a lei seja válida para todos os casos similares, não se restringindo apenas a alguns casos particulares.  A simplicidade pode ser colocada de forma matemática, dizendo que a dimensão das funções deve ser muito menor que o número de observações. Há uma ideia implícita de que se deve buscar a menor quantidade possível de parâmetros visando a simplicidade — não queremos simplesmente ajustar uma curva aos dados. Por fim, essas leis possuem um papel central ao servir como conexão teórica entre os dados observados e modelos microscópicos, que devem fornecer uma explicação mecanicista da lei e/ou ser usados em uma teoria mais geral sobre os fundamentos desse sistema.

Antes de concluir esta sessão, uma breve justificativa sobre por que temos a ideia de que existe algo que podemos generalizar como leis estatísticas. Isso decorre da observação de que diferentes leis estatísticas parecem ser motivadas, justificadas, usadas e estudadas de forma semelhante. De forma geral, a forma de raciocinar com as leis estatísticas segue uma sequência de três passos conectados.

* **Definição e validação**: Começamos com uma análise empírica. O passo inicial é, inevitavelmente, análogo a toda ciência: a análise de um grande conjunto de observações. É neste momento que observamos algum padrão nos dados e propomos uma lei estatística. Também é nesse momento que testamos a validade da lei proposta em outros conjuntos de dados. Evidentemente, esse passo passa por diversas reformulações, generalizações, modificações e toda a sorte de processos científicos tradicionais.

A partir desse passo, os próximos dois passos acontecem de forma paralela; isto é, em posse da lei estatística proposta no passo anterior, temos dois caminhos a percorrer:

* **Explicação da lei**: A explicação da lei estatística passa pelo desenvolvimento de um modelo generativo, isto é, um modelo capaz de gerar uma distribuição que obedeça à lei estatística. Após considerar que a lei estatística é empiricamente válida, uma questão óbvia a ser investigada é sua origem. Para isso, propomos modelos mecanísticos simples que gerem observações que satisfaçam as leis estatísticas.

* **Consequências da lei**: Uma segunda pergunta que surge após considerarmos que a lei estatística é válida é quais são as consequências dessa lei. Trata-se de uma extrapolação das suas consequências para além do que observamos até então — isto é, predições baseadas na própria lei, na sua relação com outras leis, no uso dessa lei como restrição em outros modelos, etc. A capacidade preditiva de uma lei é frequentemente considerada o objetivo principal em uma pesquisa tradicional.

Essencialmente, o que fazemos é associar uma lei estatística (macroscópica) necessariamente a um modelo generativo (microscópico), adotando esse método como a forma padrão de trabalho.  Podemos notar, então, que as investigações nessa área têm três elementos fundamentais:
1. Um sistema complexo a ser investigado;
2. Uma lei estatística emergente desse sistema;
3. Um modelo generativo capaz de reproduzir a lei estatística em nível macroscópico, a partir de regras microscópicas.

Eu adoto como conceito de modelagem, para a construção do modelo generativo, a modelagem baseada em agentes. Por isso, abordaremos esse conceito na próxima sessão. Também vale comentar brevemente que essa abordagem moderna de fazer ciência só se torna possível quando há dados suficientes sobre as sociedades, que nos permitam propor e testar leis estatísticas, e poder computacional suficiente para implementarmos os modelos. Ou seja, trata-se necessariamente de um fruto do nosso tempo.

## Modelagem baseada em agentes

## Fatos estilísticos

## Econofísica

O estudo das leis estatísticas remonta ao nascimento de muitas disciplinas científicas no século XVII. A origem da ideia de que diferentes conjuntos de dados e fenômenos podem ser descritos pela mesma função ou distribuição universal está intimamente relacionada com a tentativa de alargar o sucesso dos métodos quantitativos nas ciências naturais (física clássica) para as ciências biológicas e sociais através da estatística. Esta ideia desempenha um papel central nos trabalhos do cientista francês Pierre-Simon Laplace (1749-1827) e do belga Adolphe Quetelet (1796-1874) na primeira metade do século XIX.   

O termo ``física social'' ficou associado a este campo nascente de estudos sociais quantitativos, um termo também adotado pelo positivista francês Auguste Comte (1798-1857). Embora Comte tenha mais tarde transitado para os termos ``Sociologia'' e ``Ciências Sociais'', que se tornaram mais prevalentes, a ``sociofísica'' ou ``física social'' persistiu no século XX e continua a ser utilizada, frequentemente associada a modelos inspirados na física (da matéria condensada). 


[^1]: Também há um artigo de divulgação científica interessante intitulado *[A física das partículas humanas](https://cienciahoje.periodicos.capes.gov.br/storage/acervo/ch/ch_342.pdf)* (Celia Anteneodo).
[^2]: A discussão completa pode ser encontrada no livro [*Statistical Laws in Complex Systems*](https://arxiv.org/abs/2407.19874) (Altmann).
